"""
The gradio demo server for chatting with a single model.
"""

import argparse
from collections import defaultdict
import datetime
import json
import os
import random
import time
import uuid

import gradio as gr
import requests

from fastchat.serve.sys_messages import get_sys_message, get_user_message
from fastchat.conversation import SeparatorStyle
from fastchat.constants import (
    LOGDIR,
    WORKER_API_TIMEOUT,
    ErrorCode,
    MODERATION_MSG,
    CONVERSATION_LIMIT_MSG,
    SERVER_ERROR_MSG,
    INACTIVE_MSG,
    INPUT_CHAR_LEN_LIMIT,
    CONVERSATION_TURN_LIMIT,
    SESSION_EXPIRATION_TIME,
    MAX_TOKENS
)
from fastchat.model.model_adapter import get_conversation_template
from fastchat.model.model_registry import model_info
from fastchat.serve.api_provider import (
    anthropic_api_stream_iter,
    openai_api_stream_iter,
    palm_api_stream_iter,
    init_palm_chat,
)
from fastchat.utils import (
    build_logger,
    # # violates_moderation,
    get_window_url_params_js,
    parse_gradio_auth_creds,
)


logger = build_logger("gradio_web_server", "gradio_web_server.log")

headers = {"User-Agent": "FastChat Client"}

no_change_btn = gr.Button.update()
enable_btn = gr.Button.update(interactive=True)
disable_btn = gr.Button.update(interactive=False)


enable_txt = gr.Textbox.update(interactive=True)
disable_txt = gr.Textbox.update(interactive=False)

controller_url = None
enable_moderation = False

learn_more_md = """
### License
Falcon-7B-Instruct is made available under the Apache 2.0 license.
"""
# The service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.

ip_expiration_dict = defaultdict(lambda: 0)


class State:
    def _init_(self, model_name, key=None):

        self.conv = get_conversation_template(model_name)
        self.conv_id = uuid.uuid4().hex
        self.skip_next = False
        self.model_name = model_name
        self._user_message = False
        self.key = key
        if model_name == "palm-2":
            # According to release note, "chat-bison@001" is PaLM 2 for chat.
            # https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023
            self.palm_chat = init_palm_chat("chat-bison@001")
        if system_message is not None:
            self.conv.set_system_message(system_message)



    def to_gradio_chatbot(self):
        return self.conv.to_gradio_chatbot()
    def add_landing_message(self):
        user_message = get_user_message(self.key)
        # conv = state.conv
        self.conv.messages.insert(0, [self.conv.roles[0], user_message])
        self.conv.messages.insert(1, [self.conv.roles[1], None])
        self._user_message = True
    def dict(self):
        base = self.conv.dict()
        base.update(
            {
                "conv_id": self.conv_id,
                "model_name": self.model_name,
            }
        )
        return base


def set_global_vars(controller_url_, enable_moderation_):
    global controller_url, enable_moderation
    controller_url = controller_url_
    enable_moderation = enable_moderation_


def get_conv_log_filename():
    t = datetime.datetime.now()
    name = os.path.join(LOGDIR, f"{t.year}-{t.month:02d}-{t.day:02d}-conv.json")
    return name


def get_model_list(controller_url, add_chatgpt, add_claude, add_palm):
    ret = requests.post(controller_url + "/refresh_all_workers")
    assert ret.status_code == 200
    ret = requests.post(controller_url + "/list_models")
    models = ret.json()["models"]
    logger.info(f"Models: {models}")
    return models


def load_demo_single(models, url_params, request: gr.Request):
    selected_model = models[0] if len(models) > 0 else ""
    if "model" in url_params:
        model = url_params["model"]
        if model in models:
            selected_model = model
    logger.info(f"selecterd model in load_demo_single {selected_model}")

    state = add_landing_message(None, url_params)

    # state = None
    # print("-----------------------------------------", state.to_gradio_chatbot())
    print("-----------------------------------------")
    return (
        state,
        gr.Chatbot.update(visible=True),
        gr.Textbox.update(visible=True),
        gr.Button.update(visible=True),
        gr.Markdown.update(visible=True),
        state.to_gradio_chatbot() if state is not None else None,
    )


def uri_text(url_params, request): 
    text = url_params.get('question', 'Hey')
    # state = append_message(None, text)
    ret = add_text(None, text, request)
    state = ret[0]
    res = bot_response(state, request)
    for data in res:
        state = data[0]    
    return state
def load_demo(url_params, request: gr.Request):
    global models
    global sys_prmopt
    global system_message
    system_key = url_params.get('key', args.param_key)
    system_message = get_sys_message(system_key)


    ip = request.client.host
    logger.info(f"load_demo. ip: {ip}. params: {url_params}")
    ip_expiration_dict[ip] = time.time() + SESSION_EXPIRATION_TIME

    if args.model_list_mode == "reload":
        models = get_model_list(
            controller_url, args.add_chatgpt, args.add_claude, args.add_palm
        )

    return load_demo_single(models, url_params, request)


def vote_last_response(state, vote_type, model_selector, request: gr.Request):
    with open(get_conv_log_filename(), "a") as fout:
        data = {
            "tstamp": round(time.time(), 4),
            "type": vote_type,
            "model": model_selector,
            "state": state.dict(),
            "ip": request.client.host,
        }
        fout.write(json.dumps(data) + "\n")


def upvote_last_response(state, model_selector, request: gr.Request):
    logger.info(f"upvote. ip: {request.client.host}")
    vote_last_response(state, "upvote", model_selector, request)
    return ("",) + (disable_btn,) * 3


def downvote_last_response(state, model_selector, request: gr.Request):
    logger.info(f"downvote. ip: {request.client.host}")
    vote_last_response(state, "downvote", model_selector, request)
    return ("",) + (disable_btn,) * 3


def flag_last_response(state, model_selector, request: gr.Request):
    logger.info(f"flag. ip: {request.client.host}")
    vote_last_response(state, "flag", model_selector, request)
    return ("",) + (disable_btn,) * 3


def regenerate(state, request: gr.Request):
    logger.info(f"regenerate. ip: {request.client.host}")
    state.conv.update_last_message(None)
    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 5


def clear_history(request: gr.Request):
    logger.info(f"clear_history. ip: {request.client.host}")
    state = None
    return (state, [], "") + (disable_btn,) * 5

def add_landing_message(state, url_params):
    demo_key = url_params.get('key', args.param_key)


    if demo_key not in ['malaffi', 'p1', 'p2', 'p3']  :
        return state

    if state is None:
        state = State(model_name, demo_key)

    state.add_landing_message()

    # user_message = get_user_message(demo_key)
    # conv = state.conv
    # conv.append_message(conv.roles[0], user_message)
    # conv.append_message(conv.roles[1], None)


    return state
def add_text(state, text, request: gr.Request):
    ip = request.client.host
    logger.info(f"add_text. ip: {ip}. len: {len(text)}")
    logger.info(f"add_text. path_params: {request.path_params}. ")
    logger.info(f"add_text. query_params: {request.query_params}. ")
    model_selector = model_name#"llama-2-13b-chat-hf"
    if state is None:
        state = State(model_selector)

    if len(text) <= 0:
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), "") + (no_change_btn,) * 5

    if ip_expiration_dict[ip] < time.time():
        logger.info(f"inactive. ip: {request.client.host}. text: {text}")
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), INACTIVE_MSG) + (no_change_btn,) * 5

    if enable_moderation:
        flagged = False### violates_moderation(text)
        if flagged:
            logger.info(f"violate moderation. ip: {request.client.host}. text: {text}")
            state.skip_next = True
            return (state, state.to_gradio_chatbot(), MODERATION_MSG) + (
                no_change_btn,
            ) * 5

    conv = state.conv
    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_TURN_LIMIT:
        logger.info(f"conversation turn limit. ip: {request.client.host}. text: {text}")
        state.skip_next = True
        return (state, state.to_gradio_chatbot(), CONVERSATION_LIMIT_MSG) + (
            no_change_btn,
        ) * 5
    # count = conv.approximateTokenCount(text)
    # if count >= INPUT_CHAR_LEN_LIMIT
    # if state.remove_user_message and len(conv.messages) > 0:
    #     conv.messages.pop(0)
    #     conv.messages.pop(0)
    # state.remove_user_message = False

    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off
    conv.append_message(conv.roles[0], text)
    conv.append_message(conv.roles[1], None)
    return (state, state.to_gradio_chatbot(), "") + (disable_btn,) * 5


def post_process_code(code):
    sep = "\n```"
    if sep in code:
        blocks = code.split(sep)
        if len(blocks) % 2 == 1:
            for i in range(1, len(blocks), 2):
                blocks[i] = blocks[i].replace("\\", "")
        code = sep.join(blocks)
    return code


def model_worker_stream_iter(
    conv,
    model_name,
    worker_addr,
    prompt,
    temperature,
    repetition_penalty,
    top_p,
    max_new_tokens,
):
    # Make requests
    gen_params = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "repetition_penalty": repetition_penalty,
        "top_p": top_p,
        "max_new_tokens": max_new_tokens,
        "stop": conv.stop_str,
        "stop_token_ids": conv.stop_token_ids,
        "echo": False,
    }
    logger.info(f"==== request ====\n{gen_params}")

    # Stream output
    response = requests.post(
        worker_addr + "/worker_generate_stream",
        headers=headers,
        json=gen_params,
        stream=True,
        timeout=WORKER_API_TIMEOUT,
    )
    for chunk in response.iter_lines(decode_unicode=False, delimiter=b"\0"):
        if chunk:
            data = json.loads(chunk.decode())
            yield data


def bot_response(state, request: gr.Request):
    start_tstamp = time.time()
    max_new_tokens = 1024
    top_p = 1.0
    temperature = 0.01

    if state.skip_next:
        # This generate call is skipped due to invalid inputs
        state.skip_next = False
        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5
        return

    conv, model_name = state.conv, state.model_name
    if model_name == "gpt-3.5-turbo" or model_name == "gpt-4":
        prompt = conv.to_openai_api_messages()
        stream_iter = openai_api_stream_iter(
            model_name, prompt, temperature, top_p, max_new_tokens
        )
    elif model_name == "claude-2" or model_name == "claude-instant-1":
        prompt = conv.get_prompt()
        stream_iter = anthropic_api_stream_iter(
            model_name, prompt, temperature, top_p, max_new_tokens
        )
    elif model_name == "palm-2":
        stream_iter = palm_api_stream_iter(
            state.palm_chat, conv.messages[-2][1], temperature, top_p, max_new_tokens
        )
    else:
        # Query worker address
        ret = requests.post(
            controller_url + "/get_worker_address", json={"model": model_name}
        )
        worker_addr = ret.json()["address"]
        logger.info(f"model_name: {model_name}, worker_addr: {worker_addr}")

        # No available worker
        if worker_addr == "":
            conv.update_last_message(SERVER_ERROR_MSG)
            yield (
                state,
                state.to_gradio_chatbot(),
                disable_btn,
                disable_txt,
            )
            return

        from copy import deepcopy
        conv_copy = deepcopy(conv)
        if state._user_message:
            conv_copy.messages.pop(0)
            conv_copy.messages.pop(0)
        prompt = conv_copy.get_prompt()
        while len(conv_copy.messages) > 2:
        # while conv_copy.approximateTokenCount(prompt) > MAX_TOKENS and len(conv_copy.messages) > 1:
            conv_copy.remove_first_message()
            prompt = conv_copy.get_prompt()


        if "t5" in model_name:
            repetition_penalty = 1.2
        elif "medfalcon" in model_name:
            repetition_penalty = 1.5
        else:
            repetition_penalty = 1.0

        stream_iter = model_worker_stream_iter(
            conv,
            model_name,
            worker_addr,
            prompt,
            temperature,
            repetition_penalty,
            top_p,
            max_new_tokens,
        )

    conv.update_last_message("▌")
    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5

    try:
        for data in stream_iter:
            if data["error_code"] == 0:
                output = data["text"].strip()
                if "vicuna" in model_name:
                    output = post_process_code(output)
                conv.update_last_message(output + "▌")
                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5
            else:
                output = data["text"] + f"\n\n(error_code: {data['error_code']})"
                conv.update_last_message(output)
                yield (state, state.to_gradio_chatbot()) + (
                    disable_btn,
                    disable_txt,
                )
                return
            time.sleep(0.015)
    except requests.exceptions.RequestException as e:
        conv.update_last_message(
            f"{SERVER_ERROR_MSG}\n\n"
            f"(error_code: {ErrorCode.GRADIO_REQUEST_ERROR}, {e})"
        )
        yield (state, state.to_gradio_chatbot()) + (
            disable_btn,
            disable_txt
        )
        return
    except Exception as e:
        conv.update_last_message(
            f"{SERVER_ERROR_MSG}\n\n"
            f"(error_code: {ErrorCode.GRADIO_STREAM_UNKNOWN_ERROR}, {e})"
        )
        yield (state, state.to_gradio_chatbot()) + (
            disable_btn,
            disable_txt
        )
        return

    # Delete "▌"
    conv.update_last_message(conv.messages[-1][-1][:-1])
    yield (state, state.to_gradio_chatbot()) + (enable_btn, enable_txt)
    #  * 5

    finish_tstamp = time.time()
    logger.info(f"{output}")

    with open(get_conv_log_filename(), "a") as fout:
        data = {
            "tstamp": round(finish_tstamp, 4),
            "type": "chat",
            "model": model_name,
            "gen_params": {
                "temperature": temperature,
                "top_p": top_p,
                "max_new_tokens": max_new_tokens,
            },
            "start": round(start_tstamp, 4),
            "finish": round(finish_tstamp, 4),
            "state": state.dict(),
            "ip": request.client.host,
        }
        fout.write(json.dumps(data) + "\n")


block_css = """
#disclaimer_markdown {
    font-size: 85%
}
#disclaimer_markdown th {
    display: none;
}
#disclaimer_markdown td {
    padding-top: 1px;
    padding-bottom: 1px;
}
#leaderboard_markdown {
    font-size: 104%
}
#leaderboard_markdown td {
    padding-top: 1px;
    padding-bottom: 1px;
}
#leaderboard_dataframe td {
    line-height: 0.1em;
}
footer {
    visibility: hidden
}
#chatbot {
    color: #7563F7
}
.logo-container {
    padding: 0.5px;
}
.logo-med42 {
    padding: 0.5px;
    width: 50%;
    float: left;
}
.logo-m42 {
    padding: 10.5px;
    width: 50%;
    float: left;
}
.logo-m42 img {
    height: 39px;
    max-height: 100%;
    width: auto;
}
.logo-med42 img {
    height: 60px;
    max-height: 100%;
    width: auto;
}
.disclaimer-container {
    padding: 0px;
}
.disclaimer-child-text {
    padding: 0.5px;
    width: 90%;
    float: left;
}
.disclaimer-child-img {
    padding: 0px;
    width: 10%;
    float: right;
}
.disclaimer-child-img img {
    height: 20px;
    max-height: 100%;
    width: auto;
}

"""
# 60x39
# align="right" 
def build_single_model_ui(models, args):
    state = gr.State()
    m42_logo_md = """
    <div class="logo-container">
        <div align="left" class="logo-med42">
            <img src="file/Med42_Logo_White.png" alt="M42">
        </div>
        <div align="right" class="logo-m42">
            <img src="file/logo.svg" alt="Med42" >
        </div>
    </div>
    """
    # disclaimer_md = """
    #         *NOTE*: This link demonstrates a prototype of the clinical large language model (Med42) under development at M42. This link is for use by the intended recipients only and should not be shared with others. Since the Med42 is under active development, you might experience disconnection, erroneous outputs, downtime and other such issues.
            
    #         Please do not treat anything said by the Med42 as a medical advice and reach out to one of our qualified healthcare professionals at any of our network hospitals. Learn more about our hospitals at: [M42](https://www.m42.ae) <img src="file/M42LogoBlack.svg" alt="M42" width="20px"/>
    # """


    disclaimer_md = '''
    <div id="disclaimer_markdown">
            <div>
            <p>
            <strong>NOTE</strong>: This link demonstrates a prototype of the clinical large language model (Med42) under development at M42. This link is for use by the intended recipients only and should not be shared with others. Since the Med42 is under active development, you might experience disconnection, erroneous outputs, downtime and other such issues.
            </p>
            </div>
            <div class="disclaimer-child-text">
            <p>Please do not treat anything said by the Med42 as a medical advice and reach out to one of our qualified healthcare professionals at any of our network hospitals. Learn more about our hospitals at: <a href="https://www.m42.ae" target="_blank">M42</a> 
            </p>
        </div>
    </div>

    '''
    gr.Markdown(m42_logo_md, visible=True)
    chatbot = gr.Chatbot(
        elem_id="chatbot",
        label="",
        visible=False,
        height=550,
        starts_with="bot"
    )
    with gr.Row().style(equal_height=True):
        with gr.Column(scale=20):
            textbox = gr.Textbox(
                show_label=False,
                placeholder="Describe your problem here",
                visible=False,
                container=False,
                lines=2,
            )
        with gr.Column(scale=1, min_width=50):
            send_btn = gr.Button(value="Send", visible=False)

    disclaimer = gr.Markdown(disclaimer_md, elem_id="disclaimer_markdown", visible=False)
    # model_name = args.model_name
    # print(f"model_name ----  {model_name}")
    textbox.submit(
        add_text, [state, textbox], [state, chatbot, textbox]
    ).then(
        bot_response,
        [state],
        [state, chatbot, send_btn] ,
    )
    send_btn.click(
        add_text, [state, textbox], [state, chatbot, textbox]
    ).then(
        bot_response,
        [state],
        [state, chatbot, send_btn],
    )

    return state, chatbot, textbox, send_btn, disclaimer
    # model_selector , button_row, parameter_row


def build_demo(models, args):
    # theme = gr.themes.Default().set(
    # url_params = gr.JSON(visible=False)
    # print(f"getting {url_params}")
    theme = gr.themes.Base().set(
        background_fill_primary="#0d212c",
        button_primary_text_color="#FFFFFF",
        button_primary_background_fill="#7563F7",
        button_secondary_text_color="#FFFFFF",
        button_secondary_background_fill="#7563F7",
        background_fill_secondary="#202326",
        body_text_color="#FFFFFF",
        border_color_primary="#0d212c",
        color_accent_soft="#7563F7",
        input_background_fill="#202326",
        button_secondary_border_color="#7563F7",
        block_border_width="0px",
        block_label_border_width="0px",

    )
 
    with gr.Blocks(
        title="Med42",
        # theme=theme,
        theme=theme,
        # theme=gr.themes.Base(),
        css=block_css,
        # css="footer {visibility: hidden}",
        # css="footer {dispaly: none !important}",
    ) as demo:
        url_params = gr.JSON(visible=False)

        (
            state,
            chatbot,
            textbox,
            send_btn,
            disclaimer
        ) = build_single_model_ui(models, args)


        
        if args.model_list_mode not in ["once", "reload"]:
            raise ValueError(f"Unknown model list mode: {args.model_list_mode}")
        demo.load(
            load_demo,
            [url_params],
            [
                state,
                chatbot,
                textbox,
                send_btn,
                disclaimer,
                chatbot,
            ],
            _js=get_window_url_params_js,
        )

    return demo


if _name_ == "_main_":
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int)
    parser.add_argument(
        "--share",
        action="store_true",
        help="Whether to generate a public, shareable link.",
    )
    parser.add_argument(
        "--controller-url",
        type=str,
        default="http://localhost:21001",
        help="The address of the controller.",
    )
    parser.add_argument(
        "--concurrency-count",
        type=int,
        default=10,
        help="The concurrency count of the gradio queue.",
    )
    parser.add_argument(
        "--model-list-mode",
        type=str,
        default="once",
        choices=["once", "reload"],
        help="Whether to load the model list once or reload the model list every time.",
    )
    parser.add_argument(
        "--moderate", action="store_true", help="Enable content moderation"
    )
    parser.add_argument(
        "--add-chatgpt",
        action="store_true",
        help="Add OpenAI's ChatGPT models (gpt-3.5-turbo, gpt-4)",
    )
    parser.add_argument(
        "--add-claude",
        action="store_true",
        help="Add Anthropic's Claude models (claude-2, claude-instant-1)",
    )
    parser.add_argument(
        "--add-palm",
        action="store_true",
        help="Add Google's PaLM model (PaLM 2 for Chat: chat-bison@001)",
    )
    parser.add_argument(
        "--gradio-auth-path",
        type=str,
        help='Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: "u1:p1,u2:p2,u3:p3"',
        default=None,
    )
    parser.add_argument(
        "--root-path",
        type=str,
        help='Set root path to serve application from [still in test phase]"',
        default="",
    )
    parser.add_argument(
        "--param_key",
        type=str,
        help='',
        default="shifa",
    )
    
    args = parser.parse_args()
    logger.info(f"args: {args}")
    global model_name
    
    set_global_vars(args.controller_url, args.moderate)
    models = get_model_list(
        args.controller_url, args.add_chatgpt, args.add_claude, args.add_palm
    )
    model_name = models[0]
    logger.info(f"models: {models}")
    auth = [("Med42_M42_Admin", "D3M0Med!_$#V42TkFFpn#@M$#G$@"), ("user_01", "Med42@70b!"), ("user_02", "Med42@70b)@!"), ('ashish', 'akoshy24m'), ("user_03", "Med42@70b)#"), ("rabia.zahid", "Med42#70b)&"), ("wwafa", "Med42!70b)^"), ("maaz", "m3d$@!b&)^B"), ("halnowais@m42.ae", "Hasan1!"), ("vkumar@m42.ae", "Y@n93t!")]
    if args.gradio_auth_path is not None:
        auth = auth + parse_gradio_auth_creds(args.gradio_auth_path)

    # Launch the demo
    demo = build_demo(models, args)
    demo.queue(
        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False
    ).launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        max_threads=200,
        root_path=args.root_path,
        favicon_path='faviconm42.png',
        auth=auth        
    )